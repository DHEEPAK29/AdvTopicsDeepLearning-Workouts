Jan 16 25  

#### Distribution:   

<img src="https://github.com/user-attachments/assets/11a205f9-2de2-4734-b3fa-d3f40e5fbb47" alt="image" width="500">

SORA : comes from the Japanese word for "sky," symbolizing its limitless creative potential.  
SORA 3  

LLM   
|||
RNN  

### Evolution: 
Space Complexity -  Memory on Chip. 
Time Complexity - Speed up. 

Paper:
1. Vaswani, A., 2017. Attention is all you need. Advances in Neural Information Processing Systems. - [LINK](https://arxiv.org/abs/1706.03762) 

## Regression Loss Functions    
Mean Squared Error (MSE)  
Mean Absolute Error (MAE)
Huber Loss  
Log-Cosh Loss  

## Classification Loss Functions  
Binary Cross-Entropy Loss  
Categorical Cross-Entropy Loss  
Sparse Categorical Cross-Entropy Loss  
Kullback-Leibler Divergence (KL Divergence)  

## Ranking Loss Functions  
Hinge Loss  
Margin Ranking Loss  
Contrastive Loss  
Triplet Loss  

## Image Segmentation Loss Functions
Dice Loss  
Intersection Over Union (IoU) Loss  
Tversky Loss  
Focal Loss  

## GAN Loss Functions  
Minimax Loss (Standard GAN Loss)  
Wasserstein Loss  
Least Squares GAN Loss  
Hinge Loss for GANs  

## Reinforcement Learning Loss Functions
Mean Squared Bellman Error
Policy Gradient Loss
Proximal Policy Optimization (PPO) Loss
Temporal Difference (TD) Loss



### ReLU vs Sigmoid 
### MSE vs Cross-Entropy (k.. Divergence): Gemini, GPT.

## Why is L2 not a sensible distance in probability space? 
## Why to pick a specific Loss Function? 
## Cross entropy between 9.901 and 0.001 

### Qns: 

Loss Function:  
L2, Cross entropy: on prediction or reward.  

Regularization: L0/L1/l2, dropout, early stopping, SGD.  

L1 - Need to convert the denom. Applicable when operating with same param.    

Batch size - based on the Memory of GPU.  
Change of batch size: irrespective of the GPU: Different sorts of averaging.  

RMSprop vs ADAM: 
Adagrad  
L2 Norm 
Sparse features  

#### Hows Adagrad different from RMSprp:: adaptive learning rate optimization algorithms  
|||  
accumulated sum of squared gradients vs moving average of squared gradients   

Learning types   
Data modalities  


Un supervised  
Supervised  
Self Supervised   
Semi Supervised   

Autoencoder   

Retrieval Aug Generation (RAG) 

