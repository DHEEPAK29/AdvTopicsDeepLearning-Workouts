BERT 

Tokens in different models: 
GPT-2: Around 50,000 tokens.
BERT (Large, Uncased): Approximately 30,000 tokens.
T5 (Small): About 32,000 tokens.
RoBERTa (Large): Around 50,000 tokens.
XLNet (Base): Approximately 32,000 tokens.
ALBERT (Large): Around 30,000 tokens.
DistilBERT: About 30,000 tokens.
ELECTRA: Approximately 30,000 tokens.
DeBERTa: Around 50,000 tokens.
BLOOM: Approximately 50,000 tokens.

Encoder Decoder 


Attention: 
Residual nw

Regularization
Graditent Flipping 

Cloze 
Causal: pred future not the past 
GeLu: Gaussian Error Linear Unit
<img width="500" alt="Screen_Shot_2020-05-27_at_12 48 44_PM" src="https://github.com/user-attachments/assets/7aa53ae2-92c9-42f4-a389-b2657ca9af01" />

Leaky ReLu



